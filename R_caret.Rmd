---
title: "Caret Package"
author: "Pier Lorenzo Paracchini"
date: "February 9, 2016"
output: 
  html_document: 
    keep_md: yes
---

The `caret` package is a set of functions that attemt to streamline the process of creating predictive models. It includes functions to:

* preprocess the data
* split the data
* train models (fit) and test nodels
* comparison of models

`caret` acts as a **wrapper** around other packages used for predictive models.

##Packages, Installations (with Dependencies)

```{r eval = FALSE}
#The caret package
install.packages("caret", dependencies = c("Depends", "Suggests"))

#Binning Support
install.packages("Hmisc")

#Multiple plots in same grid
install.packages("gridExtra")

#Data
install.packages("ISLR")
install.packages("AppliedPredictiveModeling")
install.packages("kernlab")
```

```{r loadingRequiredPackages, echo = FALSE, message=FALSE, warning=FALSE}
require(caret)
require(Hmisc)
require(gridExtra)
require(ggplot2)
```


##The datasets  
The `ISLR::Wage` dataset:  
```{r theData1, message=FALSE, warning=FALSE, echo = FALSE}
require(ISLR)
data(Wage)
str(Wage)
summary(Wage)
```

The `iris` dataset:  
```{r theData2, message=FALSE, warning=FALSE, echo = FALSE}
data(iris)
str(iris)
summary(iris)
```

The `spam` dataset:  
```{r theData3, message=FALSE, warning=FALSE, echo = FALSE}
require(kernlab)
data(spam)
str(spam)
summary(spam)
```

#Setting the seed 
Useful to seed the seed in order to be able to replicate the running experiments.

* Often useful to set an overall seed
* Possible to set a seed for each resample (for parallel fits)

Setting the seed   
```{r collapse = TRUE}
set.seed(123)
seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds

set.seed(123)
seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds
```

Not setting the seed   
```{r collapse = TRUE}
seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds

seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds
```


#The Caret Vocabulary & Process  
##Data Splitting
Techniques that can be used to create a **training** and **testing** dataset from the vailable data.  
###Simple Splitting Based On Outcome
Create a balanced split of the data based on the outcome, keeping the overall distribution of the data (if outcome is a classifier)....  

```{r collapse = TRUE}
prop.table(table(iris$Species))

#split 75%/25% - training/ test
inTraining <- createDataPartition(iris$Species, times = 1, p = 0.75, list = FALSE)
#If list = TRUE - a listy is returned

iris.train <- iris[inTraining,]
dim(iris.train)[1]/dim(iris)[1] * 100
prop.table(table(iris.train$Species))

iris.test <- iris[-inTraining,]
dim(iris.test)[1]/dim(iris)[1] * 100
prop.table(table(iris.test$Species))
```

###Splitting Based On Predictors
The data can be splitted on the basis of teh predictor values using the _maximum dissimilarity sampling_. Note!! Dissimilarity between two sample can be  measured in a number of ways. Simplest approach is to use the distance between predictor values - if the distance is small then the 2 values are in near proximity. Larger distances are indication for dissimilarities. See `caret::maxDissim` function.

###Cross-Validation and Resampling Techniques
Cross-Validation is a model validation technique for assessing how teh results of a statistical analysis will generalize to an independent dataset - using the same dataset e.g. **training** dataset to create different sets of test/training datasets (different cross-validation rounds).

####k-fold
Creating **training** folds...  
```{r}
set.seed(32323)
folds.training <- createFolds(y = iris$Species, k = 10, list = TRUE, returnTrain = TRUE)
#return a list objects, each list element contains info on the training dataset for each specific fold
sapply(folds.training, length)
```

Creating **testing** folds...  
```{r}
set.seed(32323)
folds.testing <- createFolds(y = iris$Species, k = 10, list = TRUE, returnTrain = FALSE)
#return a list objects, each list element contains info on the testing dataset for each specific fold
sapply(folds.testing, length)
```

If `list=FALSE` then `returnTrain` is not used - the **testing** dataset is returned (which fold each observation should be, based on teh k folder).

```{r}
set.seed(32323)
folds.testing <- createFolds(y = iris$Species, k = 5, list = FALSE)
head(folds.testing)
fold1.testing.idx <- which(folds.testing == 1)
fold1.testing <- iris[fold1.testing.idx,]
head(fold1.testing)
```

###Data Splitting Applied to Time Series: TimeSlices

```{r collapse = TRUE}
set.seed(32323)
tme_index <- 1:1000 # index for the time slice
time_folds <-  createTimeSlices(y = tme_index,initialWindow = 30, horizon = 10)
names(time_folds) #List of 2 elements train and test

#Time Slice connected with the 1st fold 30 points for TRAINING/ 10 points for TESTING
time_folds$train$Training001
time_folds$test$Testing001

#Time Slice connected with the 2nd fold 30 points for TRAINING/ 10 points for TESTING
time_folds$train$Training002
time_folds$test$Testing002
```

####Playing with `fixedWindow`  
```{r collapse = TRUE}
createTimeSlices(y = 1:9, initialWindow = 5, horizon = 1, fixedWindow = FALSE)
createTimeSlices(y = 1:9, initialWindow = 5, horizon = 1, fixedWindow = TRUE)

createTimeSlices(y = 1:9, initialWindow = 5, horizon = 3, fixedWindow = TRUE)
createTimeSlices(y = 1:9, initialWindow = 5, horizon = 3, fixedWindow = FALSE)
```

####Playing with `skip`
```{r collapse = TRUE}
createTimeSlices(y = 1:15, initialWindow = 5, horizon = 3)
createTimeSlices(y = 1:15, initialWindow = 5, horizon = 3, skip = 2)
createTimeSlices(y = 1:15, initialWindow = 5, horizon = 3, skip = 3)
```
##Visualizations
**Important!** When performing exploratory analysis on the available dataset is a **good practice** to **split the available data set in a training dataset and a testing dataset** and perform the relevant exploration/ visualization on the **training** dataset.

Visual inspection/ exploration is important for building up an "initial" understanding of the available predictors, "possible" patterns and relationships. E.g. looking for skewness, outliers, ...

Things to look for  

* Imbalance in outcomes/ predictors
* Outliers
* Group of points not explained by a predictor
* Skewed predictors

###Plots
####Histograms 
Using `graphics` ...  
```{r}
hist(Wage$wage)
```

####Scatterplot Matrix (caret)
The `pairs` plot option is available for **regression** and **classification** problems.

```{r visualizationData1}
featurePlot(x=Wage[, c("age", "education", "jobclass")], y=Wage$wage, plot="pairs")
```

```{r visualizationData2}
featurePlot(x=iris[, 1:4], y=iris$Species, plot="pairs", auto.key = list(colums= 3))
```

####Scatterplot Matrix with Ellipses (caret)
The `ellipse` plot option is available for **classification** problems.

```{r visualizationData2_1}
featurePlot(x=iris[, 1:4], y=iris$Species, plot="ellipse", auto.key = list(colums= 3))
```

####Scatterplot (ggplot2)
Simple Scatterplot:  
```{r}
qplot(x=age, y=wage, data=Wage)
```

Adding the `jobclass` dimension:  
```{r}
qplot(x=age, y=wage, colour=jobclass, data=Wage)
```

Using `education` dimension and adding regression smoothers:  
```{r}
qq <- qplot(x=age, y=wage, colour=education, data=Wage)
qq + geom_smooth(method="lm", formula = y~x)
```

####Boxplot (Hmisc + ggplot2 + gridExtra)
A boxplot example using `qplot` ...  
```{r}
cutWage <- cut2(Wage$wage, g=3) #g: number of quantiles group
p1 <- qplot(x = cutWage, y = age , data=Wage, fill=cutWage, geom=c("boxplot"))
p1
```

Adding the points overlayed...  
```{r}
p2 <- qplot(x = cutWage, y = age , data=Wage, fill=cutWage, geom=c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol=2)
```

####Boxplot (caret)
A boxplot example using `featurePlot` ...  
```{r}
featurePlot(x = iris[, 1:4],
            y = iris$Species,
            plot = "box",
            ##pass in options to bwplot
            scales = list(y = list(relation="free"), x = list(rot = 90)),
            layout= c(4,1), auto.key = list(columns = 2))
```

####Density Plots  
Using `ggplot2` ...  
```{r}
qplot(x = wage, colour = education, data = Wage, geom = "density")
```

Using `caret` ...  
```{r}
featurePlot(Wage$wage, y = Wage$education, plot = "density", scales = list(x = list(relation="free"), y = list(releation="free")), adjust = 1.5, pch = "|", auto.key = list(columns = 3))
```

####Tables
```{r}
t1 <- table(cut2(Wage$wage, g=3), Wage$jobclass)
##Show table content
t1

##Show Proportion by row
prop.table(t1, 1)

##Show Proportion by col
prop.table(t1, 2)
```

##Pre-processing
__Remember!!__ When deciding the type fo pre-processing to apply to the data a good practice is to use only the __training__ dataset. __Do Not Use All of The Data Available - exlcude the testing/ validation dataset!!__

The addition, deletion or transformation of **training** dataset.

* training and test must be processed in the same way
* test transformation will likely be imperfect
* carefull when transforming factor variables

###Why pre-processing?
Sometimes it is needed to transform predictors to make them useful or valuable for the next steps.  Predictors can have different scales, outliers or predictors could be skewed...
 
```{r collapse = TRUE}
spam.inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
spam.training <- spam[spam.inTrain,]
spam.testing <- spam[-spam.inTrain,]
summary(spam.training$capitalAve)

mean(spam.training$capitalAve)
sd(spam.training$capitalAve)
```

Distribution is right skewed..    
```{r}
hist(spam.training$capitalAve, xlab = "Average No Of Capital Letter")
```

###Data Transformations For Individual Predictors

####Centering & Scaling  
Using the mean and standar deviation of a predictor to center & scale the standarized predictor. Note when standarizing we are losing interpreatbility of values.

```{r collapse = TRUE}
p_standarized.train <- spam.training$capitalAve
p_standarized.train <- (p_standarized.train - mean(p_standarized.train))/sd(p_standarized.train)
#Centering around the mean value - the mean of the standarized predictor is 0
p_standarized.train.mean <- mean(p_standarized.train)
p_standarized.train.mean
#Scaling using the sd - the sd of the standarized predictor is 1
p_standarized.train.sd <- sd(p_standarized.train)
p_standarized.train.sd
```

__When standarizing the predictor in the test set the same mean and standard deviation used to standarize the predictor in the training dataset must be used.__

```{r collapse = TRUE}
p_standarized.test <- spam.testing$capitalAve
p_standarized.test <- (p_standarized.test - mean(spam.training$capitalAve))/sd(spam.training$capitalAve)
#Centering around the mean value of training - the mean of the standarized predictor is not 0 (but should be close to)
mean(p_standarized.test)
#Scaling using the sd of the training - the sd of the standarized predictor is not 1 (but should be close to)
sd(p_standarized.test)
```

#####Using `preProcess` function  
Working on `training` dataset...

```{r collapse = TRUE}
preProcessObj <- preProcess(spam.training[,-58], method = c("center", "scale"))
p_standarized.train.all <- predict(object = preProcessObj, spam.training[,-58]) #Center And Scale all
p_standarized.train.capitalAve <- p_standarized.train.all$capitalAve

mean(p_standarized.train.capitalAve)

sd(p_standarized.train.capitalAve)
```

Applying standarization on `testing` dataset...
```{r collapse = TRUE}
#Using the preProcessObj created using the 
p_standarized.test.all <- predict(object = preProcessObj, spam.testing[,-58]) #Center And Scale all
p_standarized.test.capitalAve <- p_standarized.test.all$capitalAve

mean(p_standarized.test.capitalAve)

sd(p_standarized.test.capitalAve)
```

#####Using `preProcess` argument in the `train` function
```{r warning=FALSE, message=FALSE}
set.seed(32343)
modelFit <- train(type ~ ., data = spam.training, preProcess= c("center", "scale"), method="glm")
modelFit
```

####Transformation To Resolve Skewness
The use of the Box & Cox family of transformations based on a specific parameter (calculated from the available values). The `"BoxCox"` will estimate a Box-Cox transformation on the predictors if the data are greater than zero).

```{r collapse = TRUE}
preProcessObj <- preProcess(spam.training[,-58], method = c("BoxCox"))
p_standarized.train.all <- predict(object = preProcessObj, spam.training[,-58]) #BoxCox all
p_standarized.train.capitalAve <- p_standarized.train.all$capitalAve

preProcessObj

preProcessObj$bc

par(mfrow=c(2,2));
hist(spam.training$capitalAve); qqnorm(spam.training$capitalAve)
hist(p_standarized.train.capitalAve); qqnorm(p_standarized.train.capitalAve)
```

###Dealing with Missing Values
One popular techniuque for imputation of missing data is a K-nearest neighbor model where a missing sample is imputed by finding the samples in the training set closest to it and average the nearby points to fill in. Note the entire training set is required every time a missing value needs to be imputed.

```{r collapse= TRUE}
set.seed(13343)
##Make some values NAs
spam.training$capAveWithNAs <- spam.training$capitalAve
selectedNAs <- rbinom(n = dim(spam.training)[1], size = 1, prob = 0.05) == 1
spam.training$capAveWithNAs[selectedNAs] <- NA
summary(spam.training$capAveWithNAs)
##The modified predictor has now a certain number of elements set to NAs

##Impute & Standardize
preObj <- preProcess(spam.training[, -58], method = "knnImpute")
capAve <- predict(preObj, spam.training[,-58])$capAveWithNAs
summary(capAve)

#Standarized True Values
capAveTruth <- spam.training$capitalAve
capAveTruth <- (capAveTruth - mean(capAveTruth))/ sd(capAveTruth)

result <- cbind(imputed = capAve[selectedNAs], real = capAveTruth[selectedNAs], difference = (capAve[selectedNAs] - capAveTruth[selectedNAs]))
head(result)

quantile(result[3])
```


###Binning: making factors out of quantitative predictors (Hmisc)
An example on how to break a quantitative variables into different categories.

```{r binningExample}
cutWage <- cut2(Wage$wage, g=3) #g: number of quantiles group
table(cutWage)
```

##Model Training & Parameter Tuning
The caret package has several functions that attempt to streamline the model building and evaluation process.

The `train` function can be used to

* evaluate, using resampling, the effect of model tuning parameters on performance
* choose the "optimal" model across these parameters
* estimate model performance from a training set

[Model List & Parameters](http://topepo.github.io/caret/modelList.html)


`train` arguments/ options
```{r}
args(train.default)
```

Using the train arguments is possible to perform some basic parameter tuning e.g.

`metric` option
* Continuous outcomes
    * RMSE Root Mean Squared Error
    * Rsquared
* Categorical outcomes
    * Accuracy
    * Kappa (a measure of concordance)


Basic parameter tuning can be done using the `trainControl` function. 

`fitControl` arguments/ options
```{r}
args(trainControl)
```

For example by default simple bootstrap resampling is used for `each resampling iteration`. `trainControl` can be used to specify the type of resampling. So if a repeated k-fold resampling is to be used instead 

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
#This function will be used in the train function through the trControl argument
```

__`trainControl` resampling options__ (see documentation for more info)

* method
    * boot = bootstrapping
    * boot632 = bootstrapping with adjustment
    * cv = cross validation
    * repeatedcv = repeated cross validation
    * LOOCV = leave-one-out cross validation
* number
    * for boot/ cross validation
    * no of samples to take
* repeats
    * Number of times to repeat subsampling
    * If big - it can slow things down