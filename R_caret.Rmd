---
title: "Caret Package"
author: "Pier Lorenzo Paracchini"
date: "February 9, 2016"
output: 
  html_document: 
    keep_md: yes
---

The `caret` package is a set of functions that attemt to streamline the process of creating predictive models. It includes functions to:

* preprocess the data
* split the data
* train models (fit) and test nodels
* comparison of models

`caret` acts as a **wrapper** around other packages used for predictive models.

##Packages, Installations (with Dependencies)

```{r eval = FALSE}
#The caret package
install.packages("caret", dependencies = c("Depends", "Suggests"))

#Binning Support
install.packages("Hmisc")

#Multiple plots in same grid
install.packages("gridExtra")

#Data
install.packages("ISLR")
install.packages("AppliedPredictiveModeling")
```

```{r loadingRequiredPackages, echo = FALSE, message=FALSE, warning=FALSE}
require(caret)
require(Hmisc)
require(gridExtra)
require(ggplot2)
```


##The datasets  
The `ISLR::Wage` dataset:  
```{r theData1, message=FALSE, warning=FALSE, echo = FALSE}
require(ISLR)
data(Wage)
str(Wage)
summary(Wage)
```

The `iris` dataset:  
```{r theData2, message=FALSE, warning=FALSE, echo = FALSE}
data(iris)
str(iris)
summary(iris)
```

#Setting the seed 
Useful to seed the seed in order to be able to replicate the running experiments.

* Often useful to set an overall seed
* Possible to set a seed for each resample (for parallel fits)

Setting the seed   
```{r collapse = TRUE}
set.seed(123)
seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds

set.seed(123)
seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds
```

Not setting the seed   
```{r collapse = TRUE}
seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds

seeds <- vector(mode = "list", length = 2)
for(i in 1:2) seeds[[i]] <- sample.int(1000, 10)
seeds
```


#The Caret Vocabulary & Process  
##Data Splitting
Techniques that can be used to create a **training** and **testing** dataset from the vailable data.  
###Simple Splitting Based On Outcome
Create a balanced split of the data based on the outcome, keeping the overall distribution of the data (if outcome is a classifier)....  

```{r collapse = TRUE}
prop.table(table(iris$Species))

#split 75%/25% - training/ test
inTraining <- createDataPartition(iris$Species, times = 1, p = 0.75, list = FALSE)
#If list = TRUE - a listy is returned

iris.train <- iris[inTraining,]
dim(iris.train)[1]/dim(iris)[1] * 100
prop.table(table(iris.train$Species))

iris.test <- iris[-inTraining,]
dim(iris.test)[1]/dim(iris)[1] * 100
prop.table(table(iris.test$Species))
```

###Splitting Based On Predictors
The data can be splitted on the basis of teh predictor values using the _maximum dissimilarity sampling_. Note!! Dissimilarity between two sample can be  measured in a number of ways. Simplest approach is to use the distance between predictor values - if the distance is small then the 2 values are in near proximity. Larger distances are indication for dissimilarities. See `caret::maxDissim` function.

###Cross-Validation and Resampling Techniques
Cross-Validation is a model validation technique for assessing how teh results of a statistical analysis will generalize to an independent dataset - using the same dataset e.g. **training** dataset to create different sets of test/training datasets (different cross-validation rounds).

####k-fold
Creating **training** folds...  
```{r}
set.seed(32323)
folds.training <- createFolds(y = iris$Species, k = 10, list = TRUE, returnTrain = TRUE)
#return a list objects, each list element contains info on the training dataset for each specific fold
sapply(folds.training, length)
```

Creating **testing** folds...  
```{r}
set.seed(32323)
folds.testing <- createFolds(y = iris$Species, k = 10, list = TRUE, returnTrain = FALSE)
#return a list objects, each list element contains info on the testing dataset for each specific fold
sapply(folds.testing, length)
```

If `list=FALSE` then `returnTrain` is not used - the **testing** dataset is returned (which fold each observation should be, based on teh k folder).

```{r}
set.seed(32323)
folds.testing <- createFolds(y = iris$Species, k = 5, list = FALSE)
head(folds.testing)
fold1.testing.idx <- which(folds.testing == 1)
fold1.testing <- iris[fold1.testing.idx,]
head(fold1.testing)
```

###Data Splitting Applied to Time Series: TimeSlices

```{r collapse = TRUE}
set.seed(32323)
tme_index <- 1:1000 # index for the time slice
time_folds <-  createTimeSlices(y = tme_index,initialWindow = 30, horizon = 10)
names(time_folds) #List of 2 elements train and test

#Time Slice connected with the 1st fold 30 points for TRAINING/ 10 points for TESTING
time_folds$train$Training001
time_folds$test$Testing001

#Time Slice connected with the 2nd fold 30 points for TRAINING/ 10 points for TESTING
time_folds$train$Training002
time_folds$test$Testing002
```

####Playing with `fixedWindow`  
```{r collapse = TRUE}
createTimeSlices(y = 1:9, initialWindow = 5, horizon = 1, fixedWindow = FALSE)
createTimeSlices(y = 1:9, initialWindow = 5, horizon = 1, fixedWindow = TRUE)

createTimeSlices(y = 1:9, initialWindow = 5, horizon = 3, fixedWindow = TRUE)
createTimeSlices(y = 1:9, initialWindow = 5, horizon = 3, fixedWindow = FALSE)
```

####Playing with `skip`
```{r collapse = TRUE}
createTimeSlices(y = 1:15, initialWindow = 5, horizon = 3)
createTimeSlices(y = 1:15, initialWindow = 5, horizon = 3, skip = 2)
createTimeSlices(y = 1:15, initialWindow = 5, horizon = 3, skip = 3)
```
##Visualizations
**Important!** When performing exploratory analysis on the available dataset is a **good practice** to **split the available data set in a training dataset and a testing dataset** and perform the relevant exploration/ visualization on the **training** dataset.

Visual inspection/ exploration is important for building up an "initial" understanding of the available predictors, "possible" patterns and relationships. E.g. looking for skewness, outliers, ...

Things to look for  

* Imbalance in outcomes/ predictors
* Outliers
* Group of points not explained by a predictor
* Skewed predictors

###Plots
####Histograms 
Using `graphics` ...  
```{r}
hist(Wage$wage)
```

####Scatterplot Matrix (caret)
The `pairs` plot option is available for **regression** and **classification** problems.

```{r visualizationData1}
featurePlot(x=Wage[, c("age", "education", "jobclass")], y=Wage$wage, plot="pairs")
```

```{r visualizationData2}
featurePlot(x=iris[, 1:4], y=iris$Species, plot="pairs", auto.key = list(colums= 3))
```

####Scatterplot Matrix with Ellipses (caret)
The `ellipse` plot option is available for **classification** problems.

```{r visualizationData2_1}
featurePlot(x=iris[, 1:4], y=iris$Species, plot="ellipse", auto.key = list(colums= 3))
```

####Scatterplot (ggplot2)
Simple Scatterplot:  
```{r}
qplot(x=age, y=wage, data=Wage)
```

Adding the `jobclass` dimension:  
```{r}
qplot(x=age, y=wage, colour=jobclass, data=Wage)
```

Using `education` dimension and adding regression smoothers:  
```{r}
qq <- qplot(x=age, y=wage, colour=education, data=Wage)
qq + geom_smooth(method="lm", formula = y~x)
```

####Boxplot (Hmisc + ggplot2 + gridExtra)
A boxplot example using `qplot` ...  
```{r}
cutWage <- cut2(Wage$wage, g=3) #g: number of quantiles group
p1 <- qplot(x = cutWage, y = age , data=Wage, fill=cutWage, geom=c("boxplot"))
p1
```

Adding the points overlayed...  
```{r}
p2 <- qplot(x = cutWage, y = age , data=Wage, fill=cutWage, geom=c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol=2)
```

####Boxplot (caret)
A boxplot example using `featurePlot` ...  
```{r}
featurePlot(x = iris[, 1:4],
            y = iris$Species,
            plot = "box",
            ##pass in options to bwplot
            scales = list(y = list(relation="free"), x = list(rot = 90)),
            layout= c(4,1), auto.key = list(columns = 2))
```

####Density Plots  
Using `ggplot2` ...  
```{r}
qplot(x = wage, colour = education, data = Wage, geom = "density")
```

Using `caret` ...  
```{r}
featurePlot(Wage$wage, y = Wage$education, plot = "density", scales = list(x = list(relation="free"), y = list(releation="free")), adjust = 1.5, pch = "|", auto.key = list(columns = 3))
```

####Tables
```{r}
t1 <- table(cut2(Wage$wage, g=3), Wage$jobclass)
##Show table content
t1

##Show Proportion by row
prop.table(t1, 1)

##Show Proportion by col
prop.table(t1, 2)
```

##Pre-processing
The addition, deletion or transformation of **training** dataset.  
###Binning: making factors out of quantitative predictors (Hmisc)
An example on how to break a quantitative variables into different categories.

```{r binningExample}
cutWage <- cut2(Wage$wage, g=3) #g: number of quantiles group
table(cutWage)
```

##Model Training & Parameter Tuning
The caret package has several functions that attempt to streamline the model building and evaluation process.

The `train` function can be used to

* evaluate, using resampling, the effect of model tuning parameters on performance
* choose the "optimal" model across these parameters
* estimate model performance from a training set

[Model List & Parameters](http://topepo.github.io/caret/modelList.html)


`train` arguments/ options
```{r}
args(train.default)
```

Using the train arguments is possible to perform some basic parameter tuning e.g.

`metric` option
* Continuous outcomes
    * RMSE Root Mean Squared Error
    * Rsquared
* Categorical outcomes
    * Accuracy
    * Kappa (a measure of concordance)


Basic parameter tuning can be done using the `trainControl` function. 

`fitControl` arguments/ options
```{r}
args(trainControl)
```

For example by default simple bootstrap resampling is used for `each resampling iteration`. `trainControl` can be used to specify the type of resampling. So if a repeated k-fold resampling is to be used instead 

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
#This function will be used in the train function through the trControl argument
```

__`trainControl` resampling options (see documentation for more info)

* method
    * boot = bootstrapping
    * boot632 = bootstrapping with adjustment
    * cv = cross validation
    * repeatedcv = repeated cross validation
    * LOOCV = leave-one-out cross validation
* number
    * for boot/ cross validation
    * no of samples to take
* repeats
    * Number of times to repeat subsampling
    * If big - it can slow things down