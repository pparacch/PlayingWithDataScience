---
title: "Classification Using KNN"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

__Core Concept__  
"_... things that are alike are likely to have properties that are alike._"

Nearest-Neighbors (NN) classifiers are defined by their characteristic of classifying __unlabeled__ examples by assigning them the class of similar labeled examples. NN methods are quite owerful.

# The k-NN algorithm

| Strengths | Weakness |
| --------- | -------- |
| Simple and effective | Does not produce a model, limited interpretability |
| Makes no assumption ablout the underlying data distribution | Requires the selection of an appropiate K |
| Fast training phase | Slow classification phase |
| | Nominal features and missing data require additional processing |


The k-NN algorith uses k nearest-neighbors to classify unlabeled obseravtions. After choosing k, the algorithm requires a trining dataset made up of labeled observations. Then for each unlabeled observation, k-NN identifies the k records in the training dataset that are nearest/ similar to the observation undr analysis. The unlabeled observation is assigned the same class representing the majority of the k neighbors.

The KNN algorith is considered __lazy learning/ instance-based learning/ rote learning__ cause no abstraction/ generalization occurs. A __lazy learner__ is not learning anything, it just stores the training data. For this reason the training phase - which is not actually training anything - is very fast. On the other side the making prediction tends to be very slow having to go through the training data than using an abstraction/ generalization.

Although KNN classifiers are considered lazy, they are still quite powerful under certain circumstances.

## How to Quantify Similarity 

Similarity is measured with __distance__. A __distance function__ is a function or formula that measures the similarity between two different observations/ instances.

There are many ways to calculate the __distance__ traditionally the K-NN algorithm uses the __euclidean distance__ which is the distance that would be measured if it were possible to use a ruler between the two observations/ points. Note! the euclidean distance is measured "as the crow flies" or in other words "the shortest direct route".

{p, q} are two observations to be compared, each observation has n (quantitative) features then the euclidean distance is calculated as 

\[dist(p,q) = \sqrt{(p1-q1)^2 + (p2-q2)^2 + .. + (pn -qn)^2}\]

## Choosing the appropiate k

The decion of how many neighbors (K) to use in the algorithm influences how well the classification performs on unlabeled data (__bias-variance tradeoff__). Choosing a large k reduces the variance but increases the bias (__bias the learner__), on the other side choosing a small k (e.g. k = 1) increases the variance but reduces the bias (the learner is able to identify small and important patterns).

In practice choosing k depends on the difficulty of the concept to be learned and the number of observations in the training dataset - "_One common practice is to begin with k equal to the square root of the number of training examples_" and then test several k values on a variety of test dataset and choose the one that deliver the best classification performance.

## Preparing the data for KNN

The distance formula is highly dependent on how the features are measured (e.g scale) - the features with the dominat scales will influence the distance two observations. For this reason features are transformed to a __standard range__ before applying the algorithm.

One solution is to __rescale__ the features by shrinking or expanding their range in order to have each feature contributing the distance calculation, e.g the __min-max normalization__, the __z-score standardization__.

__min-max normalization__ for a feature x

\[x_{new} = \frac{x - min(x)}{max(x) - min(x)}\]


__z-score standardization__ for a feature x

\[x_{new} = \frac{x - \mu}{\sigma} = \frac{x - mean(x)}{stdDev(x)}\]


## The case of nominal/ qualitative data

The distance formula is not defined for nominal/ qualitative data, e.g. what is the distance between "male" and "female"? A typical solution for nominal/ qualitative features is to introduce __dummy variables__ and if necessary apply normalization.


# Example - Using k-NN to diagnose breast cancer

```{r requiredLibraries, warning=F,message=F}
require(caret)
require(class)
require(gmodels)
```

## Information about the data
The data and its relevant information are available in the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).

"_The breast cancer data includes 569 examples of cancer biopsies, each with 32 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. The diagnosis is coded as "M" to indicate malignant or "B" to indicate benign._"

```{r loadTheData}
data.ori <- read.csv("./datasets/wisc_bc_data.csv", stringsAsFactors = F, header = T)

#Split the data for exploration analysis
set.seed(19711004)
idx.train <- createDataPartition(data.ori$diagnosis, p = 0.75, list = F, times = 1)

data.train <- data.ori[idx.train,]
data.test <- data.ori[-idx.train,]
```

## Exploring the data

```{r dataStr, collapse=T}
str(data.train)
```

`diagnosis` is a qualitative feature, the __outcome/ lable__, all of the others features are quantitative. 


`id` feature is a unique identifier for each patient and it does not provide useful information - so it will be excluded from the model. Note!! __ID variables__ should alwys be excluded regardless of the machine learning method used.

```{r removeID}
data.train <- data.train[,-1]

#Same need to be done on testing dataset
data.test <- data.test[,-1]
```

```{r exploreTheData, collapse=T}
table(data.train$diagnosis)
round(prop.table(table(data.train$diagnosis)) * 100, digits = 2)

#Summary of the data
summary(data.train)
```

Looking at the summary for all of the quantitative features the __scale__ of features are different and this could create problems in teh distance calculation.

## Transforming and Normalizing the numeric data

```{r diagnosisAsFactor}
data.train$diagnosis <- factor(data.train$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))

#Same transformation applied to test dataset
data.test$diagnosis <- factor(data.test$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
```

The __min-max noirmalization__ is used to transofrming and normalizing the data.

```{r dataNormalization}

normalize <- function(x){
    return ((x - min(x))/(max(x)-min(x)))
}

data.train.norm <- as.data.frame(lapply(data.train[, 2:31], normalize))
#Same transformation done on test dataset
data.test.norm <- as.data.frame(lapply(data.test[, 2:31], normalize))

summary(data.train.norm)
```

## Training a model

Using the `knn` function in the `class` package.

```{r trainModel}
mod.prediction <- knn(train = data.train.norm, test = data.test.norm, cl = data.train[,1], k = 20)
```

## Evaluating the model performance

```{r modelEvaluation}
CrossTable(x = data.test[,1], y = mod.prediction, prop.chisq = F)
```

A total of 5 observations out of 142 were wrongly classified by the k-NN approach.


## Testing alternative values of k

```{r kSelection}
applyKnn <- function(x.data, y.data, x.label, y.label, k){
    mod.prediction <- knn(train = x.data, test = y.data, cl = x.label, k = k)
    res <- table(mod.prediction, y.label)
    return (c(res[1], res[2], res[3], res[4]))
}

RESULT <- NULL

for(k in 1:100){
    tmp <- applyKnn(x.data = data.train.norm, y.data = data.test.norm, x.label = data.train[,1], y.label = data.test[,1], k)
    RESULT <- rbind(RESULT, data.frame(TP = tmp[1], FP = tmp[2], FN = tmp[3], TN = tmp[4]))
}


plot(1:100, RESULT[,1], type = "l", ylim = c(0,100))
lines(RESULT[,2], type = "l", col = 2)
lines(RESULT[,3], type = "l",col = 3)
lines(RESULT[,4], type = "l",col = 5)

legend("topright", legend = c("TP", "FP", "FN", "TN"), col = c(1,2,3,5), lwd = 1, cex = 0.5)
```

# Reference

* Chapter 3 & 4 of ["Machine Learning with R" 2nd edition](https://www.safaribooksonline.com/library/view/machine-learning-with/9781784393908/ch03.html)